{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:  /Users/catherineslaughter/Documents/Schoolwork/Fall2022/IML/Assignment1/rawcode\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install numpy\n",
    "import os\n",
    "print(\"Current working directory: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries/packages here\n",
    "import numpy as np\n",
    "# import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful Notes:\n",
    "1. Dataset 1: a linearly separable dataset where you can test the correctness of your base learner and boosting algorithms\n",
    "   \n",
    "   300 samples 2 features\n",
    "   \n",
    "   ![dataset1.png](./dataset1.png)\n",
    "   \n",
    "   Generally speaking, your learners shall 100% correctly classify the data in dataset 1.\n",
    "\n",
    "2. Dataset 2 ~ 4 : non-linearly separable cases, applying descent boosting techniques can be beneficial\n",
    "   \n",
    "   Dataset 2: 300 samples 2 features. In comparison to the performance of your single base learner, does your boosting algorithm perferm better?\n",
    "      \n",
    "   ![dataset2.png](./dataset2.png)\n",
    "      \n",
    "   Dataset 3: 400 samples 2 features (challenging)\n",
    "\n",
    "      A good classifier shall obtain a ellipse-like decision boundary on this dataset. Can your algorithms handle this dataset? If not, can you try to give reasonable explanations?\n",
    "\n",
    "   ![dataset3.png](./dataset3.png)\n",
    "\n",
    "   Dataset 4: 3000 samples 10 features (more challenging)\n",
    "   \n",
    "      This is more or less the higher dimensional version of dataset3. We visualize the first two features of dataset 3, As it is shown in the following figure, they are non-linearly separable. \n",
    "      \n",
    "      How do your algorithms perform?\n",
    "\n",
    "   ![dataset4.png](./dataset4.png)\n",
    "\n",
    "   \n",
    "3. The data is also provided in csv format:\n",
    "   1. Feature columns and a label column \n",
    "   \n",
    "HINTs: \n",
    "1. Split the data into two parts (i.e., training data and test data).\n",
    "2. Draw decision boundary (surface) of your classifiers (on dataset 1 & 2) can be helpful.\n",
    "3. Carefully design your experiments so that you can understand the influence of increasing or decreasing some parameters (e.g., learning rate, number of base learners in boosting Alg.)\n",
    "4. Make smart implementations (e.g., vectorization using numpy to avoid some nested-loops in python), so that you can efficiently run more experiments\n",
    "5. The performance of your classifiers is not of high priority in this assignment.\n",
    "   1. The datasets are all artificially generated (toy) data, in principle, there is no need to preprocess the data.\n",
    "   2. Constructive discussions on your findings are more important. If the results are not good, try to find out the reasons.\n",
    "   3. We hope this assignment can help you fill in the gap between theory and application.\n",
    "6. You are encouraged to implement not only Adaboost but also other boosting algorithms of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the dataset\n",
    "Dataset (Numpy npz file)\n",
    "|- features (Numpy.ndarray)\n",
    "|- labels (Numpy.ndarray)\n",
    "\n",
    "The data is also provided in csv format.\n",
    "\"\"\"\n",
    "\n",
    "def load_data(file_name='./dataset1.npz'):\n",
    "    \"\"\" Load the Numpy npz format dataset \n",
    "    Args:\n",
    "        file_name (string): name and path to the dataset (dataset1.npz, dataset2.npz, dataset3.npz)\n",
    "    Returns:\n",
    "        X (Numpy.ndarray): features\n",
    "        y (Numpy.ndarray): 1D labels\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    data = np.load(file_name)\n",
    "    X, y = data['features'], data['labels']\n",
    "    return X, y\n",
    "\n",
    "# Load dataset 1 by default\n",
    "X, y =load_data()\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skeleton codes:\n",
    "You should follow the structure of this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    # Implement your base learner here\n",
    "    def __init__(self, learning_rate, max_iter, **kwargs):\n",
    "        \"\"\" Initialize the parameters here \n",
    "        Args:\n",
    "            learning_rate (float or a collection of floats): your learning rate\n",
    "            max_iter (int): the maximum number of training iterations\n",
    "            Other parameters of your choice\n",
    "\n",
    "        Examples ToDos:\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        Try different initialization strategies (as required in Question 2.3)\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        return\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\" Implement the training strategy here\n",
    "        Args:\n",
    "            X (Numpy.ndarray, list, etc.): The training data\n",
    "            y (Numpy.ndarray, list, etc.): The labels\n",
    "            Other parameters of your choice\n",
    "\n",
    "        Example ToDos:\n",
    "        # for _ in range(self.max_iter):\n",
    "        #     Update the parameters of Perceptron according to the learning rate (self.learning_rate) and data (X, y)\n",
    "        \"\"\" \n",
    "        self.w = np.zeros((X.shape[1], 1))  # Initialize the weights to zero\n",
    "        self.b = 1  # Initialize bias with 1\n",
    "        self.any_false = True\n",
    "        self.iter = 0\n",
    "        \n",
    "        curr_lr = self.learning_rate\n",
    "        # Count the number of samples not correctly classified\n",
    "        while self.any_false and self.iter < self.max_iter:\n",
    "            print(curr_lr)\n",
    "            mis_classified_number = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                X_i = X[i]\n",
    "                y_i = y[i]\n",
    "                classify_result = np.dot(self.w.T, X_i.T) + self.b\n",
    "                if y_i * classify_result < 0:  # This means sample is misclassified\n",
    "                    self.w += curr_lr * np.dot(X_i, y_i).reshape(2, 1)\n",
    "                    self.b += curr_lr * y_i\n",
    "                    mis_classified_number += 1\n",
    "            if mis_classified_number == 0:\n",
    "                self.any_false = False  # If there isn't any sample misclassified, then the loop will end\n",
    "            else:\n",
    "                self.any_false = True  # If there is still any misclassified sample, then the loop kees running\n",
    "            self.iter += 1\n",
    "            curr_lr = self.learning_rate/self.iter\n",
    "        #print(self.w)\n",
    "        #print(self.b)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def predict(self, x, **kwargs) -> np.ndarray:\n",
    "        \"\"\" Implement the prediction strategy here\n",
    "        Args:\n",
    "            x (Numpy.ndarray, list, Numpy.array, etc.): The input data\n",
    "            Other parameters of your choice\n",
    "        Return(s):\n",
    "            The prediction value(s), namely, class label(s), others of your choice\n",
    "        \"\"\" \n",
    "        \n",
    "        labels = np.zeros(x.shape[0]).astype(int)\n",
    "        for i, data_point in enumerate(x):\n",
    "            temp = np.dot(self.w.T, data_point.T) + self.b\n",
    "            if temp < 0:\n",
    "                labels[i] = -1\n",
    "            else:\n",
    "                labels[i] = 1\n",
    "\n",
    "\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingAlgorithm:\n",
    "    # Implement your boosting algorithm here\n",
    "    def __init__(self, n_estimators, **kwargs):\n",
    "        \"\"\" Initialize the parameters here \n",
    "        Args:\n",
    "            n_estimators (int): number of base perceptron models\n",
    "            Other parameters of your choice\n",
    "        \n",
    "        Think smartly on how to utilize multiple perceptron models\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        \"\"\" Implement the training strategy here\n",
    "        Args:\n",
    "            X (Numpy.ndarray, list, etc.): The training data\n",
    "            y (Numpy.ndarray, list, etc.): The labels\n",
    "            Other parameters of your choice\n",
    "        \"\"\" \n",
    "        pass\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        \"\"\" Implement the prediction strategy here\n",
    "        Args:\n",
    "            x (Numpy.ndarray, list, Numpy.array, etc.): The input data\n",
    "            Other parameters of your choice\n",
    "        Return(s):\n",
    "            The prediction value, namely, class label(s)\n",
    "        \"\"\" \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(**kwargs):\n",
    "    \"\"\" Single run of your classifier\n",
    "    # Load the data\n",
    "    X, y = load_data()\n",
    "    # Find a way to split the data into training and test sets\n",
    "    -> X_train, y_train, X_test, y_test\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    base = Perceptron(\"your parameters\")\n",
    "    \n",
    "    # Train the classifier\n",
    "    base.fit(X_train, y_train, \"other parameters\")\n",
    "   \n",
    "    # Test and score the base learner using the test data\n",
    "    y_pred = base.predict(X_test, \"other parameters\")\n",
    "    score = SCORING(y_pred, y_test)\n",
    "    \"\"\"\n",
    "    X, y = load_data('./dataset2.npz')\n",
    "    X_train, y_train = X[0:250], y[0:250]\n",
    "    X_test, y_test = X[251:300], y[251:300]\n",
    "    \n",
    "    \n",
    "    base = Perceptron(1, 100)\n",
    "    \n",
    "    base.fit(X_train, y_train)\n",
    "    output = base.predict(X_test)\n",
    "    \n",
    "    print(output)\n",
    "    print(y_test)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1.0\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.25\n",
      "0.2\n",
      "0.16666666666666666\n",
      "0.14285714285714285\n",
      "0.125\n",
      "0.1111111111111111\n",
      "0.1\n",
      "0.09090909090909091\n",
      "0.08333333333333333\n",
      "0.07692307692307693\n",
      "0.07142857142857142\n",
      "0.06666666666666667\n",
      "0.0625\n",
      "0.058823529411764705\n",
      "0.05555555555555555\n",
      "0.05263157894736842\n",
      "0.05\n",
      "0.047619047619047616\n",
      "0.045454545454545456\n",
      "0.043478260869565216\n",
      "0.041666666666666664\n",
      "0.04\n",
      "0.038461538461538464\n",
      "0.037037037037037035\n",
      "0.03571428571428571\n",
      "0.034482758620689655\n",
      "0.03333333333333333\n",
      "0.03225806451612903\n",
      "0.03125\n",
      "0.030303030303030304\n",
      "0.029411764705882353\n",
      "0.02857142857142857\n",
      "0.027777777777777776\n",
      "0.02702702702702703\n",
      "0.02631578947368421\n",
      "0.02564102564102564\n",
      "0.025\n",
      "0.024390243902439025\n",
      "0.023809523809523808\n",
      "0.023255813953488372\n",
      "0.022727272727272728\n",
      "0.022222222222222223\n",
      "0.021739130434782608\n",
      "0.02127659574468085\n",
      "0.020833333333333332\n",
      "0.02040816326530612\n",
      "0.02\n",
      "0.0196078431372549\n",
      "0.019230769230769232\n",
      "0.018867924528301886\n",
      "0.018518518518518517\n",
      "0.01818181818181818\n",
      "0.017857142857142856\n",
      "0.017543859649122806\n",
      "0.017241379310344827\n",
      "0.01694915254237288\n",
      "0.016666666666666666\n",
      "0.01639344262295082\n",
      "0.016129032258064516\n",
      "0.015873015873015872\n",
      "0.015625\n",
      "0.015384615384615385\n",
      "0.015151515151515152\n",
      "0.014925373134328358\n",
      "0.014705882352941176\n",
      "0.014492753623188406\n",
      "0.014285714285714285\n",
      "0.014084507042253521\n",
      "0.013888888888888888\n",
      "0.0136986301369863\n",
      "0.013513513513513514\n",
      "0.013333333333333334\n",
      "0.013157894736842105\n",
      "0.012987012987012988\n",
      "0.01282051282051282\n",
      "0.012658227848101266\n",
      "0.0125\n",
      "0.012345679012345678\n",
      "0.012195121951219513\n",
      "0.012048192771084338\n",
      "0.011904761904761904\n",
      "0.011764705882352941\n",
      "0.011627906976744186\n",
      "0.011494252873563218\n",
      "0.011363636363636364\n",
      "0.011235955056179775\n",
      "0.011111111111111112\n",
      "0.01098901098901099\n",
      "0.010869565217391304\n",
      "0.010752688172043012\n",
      "0.010638297872340425\n",
      "0.010526315789473684\n",
      "0.010416666666666666\n",
      "0.010309278350515464\n",
      "0.01020408163265306\n",
      "0.010101010101010102\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck with the assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "90d3069fe3c3b20e4fbf57bf0cb04fe664322e8b41cb56c6c22d386a330b3fc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
